{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A single species, single season neural occupancy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have `n` spatial locations, each surveyed `k` times, where each location has some continuous covariate value (e.g., an environmental feature) `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "k = 20\n",
    "x = torch.distributions.uniform.Uniform(low=-torch.ones(n), high=torch.ones(n)).sample()\n",
    "x, _ = torch.sort(x)\n",
    "x = x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the detection probability `p`, and the occupancy probability `psi` are polynomial functions of $x$ on the logit (sigmoid) scale. \n",
    "Then, simulate true presence/absence states `z`, and binomial observations `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dist = torch.distributions.normal.Normal(loc=0, scale=1)\n",
    "p = torch.sigmoid(z_dist.sample() + z_dist.sample() * x + z_dist.sample() * x**2)\n",
    "psi = torch.sigmoid(z_dist.sample() + z_dist.sample() * x + z_dist.sample() * x**2)\n",
    "z = torch.distributions.bernoulli.Bernoulli(probs = psi).sample()\n",
    "y = torch.distributions.binomial.Binomial(total_count=k, probs=p * z).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the relationship between the environmental covariate and occupancy and detection probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x.numpy(), z.numpy(), alpha=.5)\n",
    "plt.plot(x.numpy(), psi.numpy(), color='r')\n",
    "plt.title('Occupancy probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x.numpy(), 1 * y.numpy() > 0, alpha=.5)\n",
    "plt.plot(x.numpy(), p.numpy(), color='r')\n",
    "plt.title('Detection probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a model\n",
    "\n",
    "The model for occupancy probabilities $\\psi$ and detection probabilities $p$ is parameterized as a neural network.\n",
    "\n",
    "A forward pass of the model is:\n",
    "\n",
    "$x_1 \\rightarrow h^{(1)}_{1:64} \\rightarrow (\\psi_1, p_1),$\n",
    "\n",
    "where subscripts inidicate the dimensionality, $x$ is input, $h^{(1)}$ is the first hidden layer, and the output is a tuple containing occupancy probability $\\psi$ and detection probability $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.fc1(x))\n",
    "        output = torch.sigmoid(self.fc2(x))\n",
    "        psi = output[:, [0]]\n",
    "        p = output[:, [1]]\n",
    "        return psi, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model object, and send it to the device (the GPU, if one is available). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "First, set up an empty list to store the loss (negative log likelihood) at each training iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, choose an optimizer, a value for the $L_2$ norm penalty on the parameters (the `weight_decay` argument), and the number of times to iterate over the dataset (`n_epoch`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), weight_decay=1e-8)\n",
    "n_epoch = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models is easier in Pytorch when a `DataLoader` object is defined. In this case, we will create a `DataLoader` with settings defined in the `params` dictionary:\n",
    "\n",
    "- `batch_size`: the number of examples to load on each training iteration\n",
    "- `shuffle`: whether to shuffle the examples for each epoch\n",
    "- `num_workers`: the number of CPU cores to use to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 32,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1}\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, iterate over training epochs, and within epochs, over minibatches. This training loop includes calculations of the negative log likelihood (`loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(n_epoch)):\n",
    "    for i_batch, xy in enumerate(dataloader):\n",
    "        # load a minibatch\n",
    "        x_i, y_i = xy\n",
    "        x_i = x_i.to(device)\n",
    "        y_i = y_i.to(device)\n",
    "        \n",
    "        # zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # determine for each example whether we know if z = 1\n",
    "        definitely_present = (y_i > 0).to(device, dtype=torch.float32)\n",
    "        maybe_absent = (y_i == 0).to(device, dtype=torch.float32)\n",
    "\n",
    "        # generate estimates of psi and p from teh model\n",
    "        psi_i, p_i = net(x_i)\n",
    "\n",
    "        # compute the loss (negative log likelihood)\n",
    "        y_dist_if_present = torch.distributions.binomial.Binomial(total_count=k, probs=p_i)\n",
    "        lp_present = torch.log(psi_i) + y_dist_if_present.log_prob(y_i)\n",
    "        lp_maybe_absent = torch.logsumexp(torch.cat((lp_present, torch.log(1 - psi_i)), dim=1), \n",
    "                                          dim=1)\n",
    "        log_prob = definitely_present * lp_present + maybe_absent * lp_maybe_absent\n",
    "\n",
    "        loss = -torch.mean(log_prob)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(running_loss)), running_loss, c='k')\n",
    "plt.xlabel(\"Number of minibatches\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "\n",
    "Now, let's see what the model predicts as a function of `x` for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_hat, p_hat = net(x.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the true parameter values from the generative model will be shown in black, and predicted parameter values from the model in red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x.numpy(), z.numpy(), alpha=.1)\n",
    "plt.scatter(x.numpy(), psi_hat.cpu().detach().numpy(), color='r', alpha=.5)\n",
    "plt.plot(x.numpy(), psi.numpy(), color='k')\n",
    "plt.title('Occupancy probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x.numpy(), y.numpy() / k, alpha = .1, color = 'b')\n",
    "plt.scatter(x.numpy(), p_hat.cpu().detach().numpy(), alpha=.5, color='r')\n",
    "plt.plot(x.numpy(), p.numpy(), color='k')\n",
    "plt.title('Detection probabilities')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
