{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A neural N-mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate data at `nsite` sites, with `nrep` repeat surveys. Here it's assumed that there is one continuous site-level covariate $x$ that has some nonlinear relationship with the expected number of individuals at a site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsite = 300\n",
    "nrep = 5\n",
    "x = np.linspace(-5, 5, nsite, dtype=np.float32).reshape(-1,1)\n",
    "\n",
    "# Draw f(x) from a Gaussian process\n",
    "def kernel(x, theta):\n",
    "    m, n = np.meshgrid(x, x)\n",
    "    sqdist = abs(m-n)**2\n",
    "    return np.exp(- theta * sqdist)\n",
    "\n",
    "K = kernel(x, theta=.1)\n",
    "L = np.linalg.cholesky(K + 1e-5* np.eye(nsite))\n",
    "f_prior = np.dot(L, np.random.normal(size=(nsite, 1)))\n",
    "\n",
    "# plot kernel\n",
    "plt.imshow(K)\n",
    "plt.show()\n",
    "\n",
    "# plot function\n",
    "plt.plot(x, f_prior)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate true abundance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 3\n",
    "lam = np.exp(f_prior + offset)\n",
    "n = np.random.poisson(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, n, c='k', alpha=.3)\n",
    "plt.plot(x, lam)\n",
    "plt.xlabel('Covariate value')\n",
    "plt.ylabel('True abundance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate count observations\n",
    "\n",
    "For simplicity, assume that the probability of detection is constant across all sites and independent of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_detection = np.array([.4], dtype=np.float32)\n",
    "y = np.random.binomial(n=n, p=pr_detection, size=(nsite, nrep)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed abundance is some fraction of the true abundance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, lam)\n",
    "for i in range(nrep):\n",
    "    plt.scatter(x, y[:, i], c='k', alpha=.3)    \n",
    "plt.xlabel('Covariate value')\n",
    "plt.ylabel('Observed abundance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMixture(nn.Module):\n",
    "    \"\"\" Neural N-mixture model \n",
    "    \n",
    "    This is a neural network that ingests x and outputs:\n",
    "    - lam(bda): expected abundance\n",
    "    - p: detection probability\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(NMixture, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        output = self.fc3(x)\n",
    "        lam = torch.exp(output[:, [0]])\n",
    "        p = torch.sigmoid(output[:, [1]])\n",
    "        return lam, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NMixture()\n",
    "net.to(device)\n",
    "running_loss = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "To train the model, create a Pytorch DataLoader, set up an optimizer, and then iterate over the dataset `n_epoch` times, training via stochastic minibatch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeDataset(Dataset):\n",
    "    \"\"\" A Datset class for simulated data\"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return x[idx, :], y[idx, :]\n",
    "\n",
    "# create a DataLoader to load training examples\n",
    "dataloader = DataLoader(FakeDataset(x, y), \n",
    "                        batch_size=256,\n",
    "                        shuffle=True, \n",
    "                        num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), weight_decay=1e-4)\n",
    "\n",
    "# generate a set of n values over which to enumerate in the likelihood\n",
    "n_max = int(np.max(y) * 5)\n",
    "n_vals = torch.from_numpy(np.arange(start=0, stop=n_max, dtype=np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid = n_vals.unsqueeze(0).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(n_epoch)):\n",
    "    for i_batch, xy in enumerate(dataloader):\n",
    "        x_i, y_i = xy\n",
    "        x_i = x_i.to(device)\n",
    "        y_i = y_i.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        lam_i, p_i = net(x_i)\n",
    "\n",
    "        # compute disribution of unobserved true abundance (n)\n",
    "        dist_n = torch.distributions.poisson.Poisson(lam_i)\n",
    "        lp_n = dist_n.log_prob(n_vals)\n",
    "        \n",
    "        # compute distribution of observations (y)\n",
    "        p_grid = p_i.unsqueeze(-1).expand(-1, 1, nrep) # (batch_size, n_max, nrep)\n",
    "        dist_y = torch.distributions.binomial.Binomial(n_grid, probs=p_grid)\n",
    "\n",
    "        # sum over repeat surveys to get the probability of y (batch_size, n_max)\n",
    "        lp_y = dist_y.log_prob(y_i.unsqueeze(1).repeat(1, n_max, 1)).sum(dim=2) # sum over repeat surveys\n",
    "\n",
    "        # multiply [y | N=n] * [N = n], yielding shape (batch_size, n_max), then sum over n\n",
    "        log_prob = torch.logsumexp(lp_n + lp_y, 1)\n",
    "\n",
    "        loss = -torch.mean(log_prob)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(running_loss)), np.log(running_loss), alpha=.1, c='k')\n",
    "plt.xlabel(\"Number of training updates\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did the model estimate about the relationship between $x$ and abundance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_hat, p_hat = net(torch.from_numpy(x).to(device))\n",
    "lam_hat = lam_hat.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, n, c='k', alpha=.3)\n",
    "plt.plot(x, lam)\n",
    "plt.plot(x, lam_hat)\n",
    "plt.xlabel('Covariate value')\n",
    "plt.ylabel('Abundance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did the model estimate about the relationship between $x$ and detection?\n",
    "\n",
    "Recall that the detection probability was fixed, and not a function of $x$ in the generative simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, pr_detection * np.ones_like(x))\n",
    "plt.plot(x, p_hat.cpu().detach().numpy())\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Covariate value')\n",
    "plt.ylabel('Detection probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
